{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 0.5])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit([[0, 0], [1, 1], [2, 2]], [0, 1, 2])\n",
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07209302325581396"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.coef_\n",
    "reg.intercept_\n",
    "reg = linear_model.RidgeCV(alphas=[0.1, 1.0, 10.0])\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.alpha_\n",
    "reg = linear_model.Ridge(alpha=0.1)\n",
    "reg.fit([[0, 0], [0, 0], [1, 1]], [0, .1, 1])\n",
    "reg.coef_\n",
    "reg.intercept_\n",
    "\n",
    "# prevent overfitting by penalizing large coefficients, which can improve the model's performance on unseen data. The penalty term is controlled by a hyperparameter, often denoted as ùõºŒ±.\n",
    "\n",
    "# Ridge Regression:\n",
    "# Ridge regression is a type of linear regression that adds a penalty term to the loss function to prevent overfitting. This penalty term is proportional to the square of the magnitude of the coefficients. The penalty term is controlled by a hyperparameter, often denoted as ùõºŒ±.\n",
    "\n",
    "# Ridge regression is useful when dealing with datasets where the number of features is greater than the number of samples. It helps in reducing the impact of multicollinearity among the features and stabilizes the model's coefficients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "\n",
    "from sklearn import linear_model\n",
    "reg = linear_model.Lasso(alpha=0.1)\n",
    "reg.fit([[0, 0], [1, 1]], [0, 1])\n",
    "reg.predict([[1, 1]])\n",
    "\n",
    "# The alpha parameter controls the strength of the L1 regularization applied to the model, which helps in feature selection by shrinking some coefficients to zero.\n",
    "\n",
    "# Lasso Regression:\n",
    "# Lasso (Least Absolute Shrinkage and Selection Operator) regression is another type of linear regression that adds a penalty term to the loss function. This penalty term is proportional to the absolute value of the coefficients. The penalty term is controlled by a hyperparameter,\n",
    "\n",
    "# Lasso regression is useful when dealing with datasets where the number of features is greater than the number of samples. It helps in selecting the most important features and performing feature selection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
